{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9Sni7Xw/18VbrRXmwmy4S"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "First, go into your terminal and if you haven’t installed Scrapy then install it using:"
      ],
      "metadata": {
        "id": "mgSdP_SJoaRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "````\n",
        "pip install scrapy\n",
        "````"
      ],
      "metadata": {
        "id": "dKDAy53gp3Pd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now from your terminal go into the directory where you want to start your project and run:"
      ],
      "metadata": {
        "id": "qGk3WoC1obFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "````\n",
        "scrapy startproject AmazonScrap\n",
        "````"
      ],
      "metadata": {
        "id": "frMjVa5Zp8Wc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now create an “amazon_scraping.py” file in spiders directory and start coding"
      ],
      "metadata": {
        "id": "pBbYIPXAoZca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "from scrapy import Request\n",
        "from scrapy.spiders import CrawlSpider, Rule\n",
        "from scrapy.linkextractors import LinkExtractor"
      ],
      "metadata": {
        "id": "X3P8dyKpoXid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a python class defining all the variables that we want to scrape"
      ],
      "metadata": {
        "id": "OYXHvsy2qokA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyItem(scrapy.Item):\n",
        "    names = scrapy.Field()\n",
        "    reviewerLink = scrapy.Field()\n",
        "    reviewTitles = scrapy.Field()\n",
        "    reviewBody = scrapy.Field()\n",
        "    verifiedPurchase = scrapy.Field()\n",
        "    postDate = scrapy.Field()\n",
        "    starRating = scrapy.Field()\n",
        "    helpful = scrapy.Field()\n",
        "    nextPage = scrapy.Field(default = 'null')"
      ],
      "metadata": {
        "id": "S5x9Ymedojw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the main class on which Scrapy will come to scrape the data"
      ],
      "metadata": {
        "id": "UMGyIp0-qsiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReviewspiderSpider(scrapy.Spider):\n",
        "    name = 'reviewspider'\n",
        "    allowed_domains = [\"amazon.com\"]\n",
        "    start_urls = [\"<Any Product URL u wanna scrape>\"]"
      ],
      "metadata": {
        "id": "WPXBXXlBopdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the same class define a function that will be used to scrape the link you mentioned above to get the link of “all reviews tag” on the Amazon page"
      ],
      "metadata": {
        "id": "W2gm3j89qzTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse(self, response):\n",
        "  # This will get the link for the all reviews tag on amazon page.\n",
        "  all_reviews = response.xpath('//div[@data-hook=\"reviews-medley-footer\"]//a[@data-hook=\"see-all-reviews-link-foot\"]/@href').extract_first()\n",
        "  # This will tell scrapy to move to all reviews page for further scraping.\n",
        "  yield response.follow(\"https://www.amazon.com\"+all_reviews, callback=self.parse_page)"
      ],
      "metadata": {
        "id": "V9QGPgLfosWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Scrapy is on the “all reviews page” of amazon, so now we will write a function that will scrape that page for all the above-mentioned items and store it in a JSON file"
      ],
      "metadata": {
        "id": "5IMDKzB5q65T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_page(self, response):\n",
        "  \n",
        "  # Scraping all the items for all the reviewers mentioned on that Page\n",
        "  \n",
        "  names=response.xpath('//div[@data-hook=\"review\"]//span[@class=\"a-profile-name\"]/text()').extract()\n",
        "  reviewerLink=response.xpath('//div[@data-hook=\"review\"]//a[@class=\"a-profile\"]/@href').extract()\n",
        "  reviewTitles=response.xpath('//a[@data-hook=\"review-title\"]/span/text()').extract()\n",
        "  reviewBody=response.xpath('//span[@data-hook=\"review-body\"]/span').xpath('normalize-space()').getall()\n",
        "  verifiedPurchase=response.xpath('//span[@data-hook=\"avp-badge\"]/text()').extract()\n",
        "  postDate=response.xpath('//span[@data-hook=\"review-date\"]/text()').extract()\n",
        "  starRating=response.xpath('//i[@data-hook=\"review-star-rating\"]/span[@class=\"a-icon-alt\"]/text()').extract()\n",
        "  helpful = response.xpath('//span[@class=\"cr-vote\"]//span[@data-hook=\"helpful-vote-statement\"]/text()').extract()\n",
        "  \n",
        "  # Extracting details of each reviewer and storing it in in the MyItem object items and then appending it to the JSON file.\n",
        "  \n",
        "  for (name, reviewLink, title, Review, Verified, date, rating, helpful_count) in zip(names, reviewerLink, reviewTitles, reviewBody, verifiedPurchase, postDate, starRating, helpful):\n",
        "      \n",
        "      # Getting the Next Page URL for futher scraping.\n",
        "      next_urls = response.css('.a-last > a::attr(href)').extract_first()\n",
        "      \n",
        "      yield MyItem(names=name, reviewerLink = reviewLink, reviewTitles=title, reviewBody=Review, verifiedPurchase=Verified, postDate=date, starRating=rating, helpful=helpful_count, nextPage=next_urls)\n"
      ],
      "metadata": {
        "id": "8Aws6dN5oy3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have got all the items and have been appended to JSON file and now its time to tell Scrapy to go to the next page and repeat the above process."
      ],
      "metadata": {
        "id": "DiYeAg2RrEvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This will get the next psge URL\n",
        "next_page = response.css('.a-last > a::attr(href)').extract_first()\n",
        "  # Checking if next page is not none then loop back in the same function with the next page URL.\n",
        "if next_page is not None:\n",
        "  yield response.follow(\"https://www.amazon.com\"+next_page, callback=self.parse_page)"
      ],
      "metadata": {
        "id": "noqD5MtMo21_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### It's time to run the Code"
      ],
      "metadata": {
        "id": "pMWVVL1kplxk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go into the AmazonScrap directory and run the following command in the terminal"
      ],
      "metadata": {
        "id": "3-xU2_jVrXgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "````\n",
        "scrapy crawl reviewspider -t json -o outputfile.json\n",
        "````"
      ],
      "metadata": {
        "id": "_sUWNDqLpsTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this, you should see a file name ````<outputfile.json>```` created in the AmazonScrap folder having all the scraped data"
      ],
      "metadata": {
        "id": "AmFfC61RrgSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may be getting 503 Service Unavailable in the terminal, this is because we may be putting to much load on the servers, to resolve that go to AmazonScrap/settings.py and add the following code and then try running"
      ],
      "metadata": {
        "id": "_cMiNQbUrw3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "````\n",
        "DOWNLOAD_TIMEOUT = 540\n",
        "DOWNLOAD_DELAY = 5\n",
        "DEPTH_LIMIT = 10\n",
        "EXTENSIONS = {\n",
        "    'scrapy.extensions.telnet.TelnetConsole': None,\n",
        "    'scrapy.extensions.closespider.CloseSpider': 1\n",
        "}\n",
        "````"
      ],
      "metadata": {
        "id": "OQrUWrJPr3jO"
      }
    }
  ]
}